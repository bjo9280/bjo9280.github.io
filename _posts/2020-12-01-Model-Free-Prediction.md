---
title: "4ê°• Model-Free Prediction"
date: 2020-12-01 00:00:00 +0900
categories: RL ReinforcementLearning
---

> 4ì¥ì€ Dynamic programming ë°©ë²•ìœ¼ë¡œ MDPì—ì„œ planningí•˜ëŠ” ë°©ë²•
>
> https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.htmlì—ì„œ ì˜ˆì‹œë¥¼ ì ê³ 

# Model-Free Reinforcement Learning

* MDPë¥¼ ëª¨ë¥´ëŠ” ìƒí™©ì—ì„œ í™˜ê²½ê³¼ ì§ì ‘ì ìœ¼ë¡œ ìƒí˜¸ì‘ìš©ì„ í•˜ë©´ì„œ ê²½í—˜ì„ í†µí•´ í•™ìŠµ í•˜ê²Œ ë˜ëŠ” ë°©ì‹



# Monte-Carlo Reinforcement Learning

* MCëŠ” ê²½í—˜ìœ¼ë¡œë¶€í„° ì§ì ‘ ë°°ìš°ëŠ” ë°©ë²•ë¡ 
* Model free ë°©ë²•ë¡ 
  * MDPì˜ ìƒíƒœ ì „ì´ë‚˜ ë³´ìƒ í•¨ìˆ˜ì— ê´€í•œ ì •ë³´ê°€ í•„ìš” ì—†ìŒ
* ì™„ì „í•œ ì—í”¼ì†Œë“œë¡œë¶€í„° ë°°ì›€
  * ì—í”¼ì†Œë“œê°€ ëë‚˜ì•¼ ë°°ìš¸ ìˆ˜ ìˆë‹¤.
* ê°„ë‹¨í•œ ì•„ì´ë””ì–´
  * ê°€ì¹˜ = í‰ê·  ë¦¬í„´

# Monte-Carlo Policy Evaluation 

ëª©í‘œ: Policyë¥¼ ì´ìš©í•´ ì–»ì€ ì—í”¼ì†Œë“œë“¤ë¡œ ë¶€í„° ê°€ì¹˜ í•¨ìˆ˜ ğ‘‰<sub>Ï€</sub>  í•™ìŠµ

![fig](https://bjo9280.github.io/assets/images/2020-12-01/mc_policy-evaluation1.png)

ë¦¬í„´ì€ ëˆ„ì ëœ ë³´ìƒì˜ í•©

![fig](https://bjo9280.github.io/assets/images/2020-12-01/mc_policy-evaluation2.png)

Value functionì€ ë¦¬í„´ì˜ ê¸°ëŒ“ê°’ ì„ì„ ê¸°ì–µ

![fig](https://bjo9280.github.io/assets/images/2020-12-01/mc_policy-evaluation3.png)



Monte-carlo policy evaluationì€ ê¸°ëŒ“ê°’ ëŒ€ì‹ ì— ì‹¤ì œ ë¦¬í„´ì˜ í‰ê· ì„ ì‚¬ìš©

# Monte-Carlo Policy Evaluation

1. ìƒíƒœ sì˜ ê°€ì¹˜ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ì„œ
2. ì—í”¼ì†Œë“œ ì•ˆì—ì„œ ìƒíƒœ së¥¼ ë°©ë¬¸í•  ë•Œ ë§ˆë‹¤
3. ì¹´ìš´í„°ë¥¼ ì¦ê°€ì‹œí‚¤ê³  N(s) â† N(s) + 1 
4. ì´ ë¦¬í„´ ê°’ë„ ì¦ê°€ì‹œí‚¤ê³  S(s) â† S(s) + G 
5. ê°€ì¹˜ëŠ” ê·¸ í‰ê· ìœ¼ë¡œ ê³„ì‚° V(s) = S(s)/N(s) 
6. ëŒ€ìˆ˜ì˜ ë²•ì¹™ì— ì˜í•´ N(s) -> âˆ ì´ë©´ V(s) -> ğ‘‰<sub>Ï€(s)</sub> 

# Example: Monte-Carlo Policy Evaluation(1)

![fig](https://bjo9280.github.io/assets/images/2020-12-01/ex_mc_policy_evaluation.gif)



# Example: Monte-Carlo Policy Evaluation(2)

![fig](https://bjo9280.github.io/assets/images/2020-12-01/ex_mc_policy_evaluation.png)

the state(1, 1) is : (0.27+0.27-0.79)/3=-0.08

# Incremental Mean

![fig](https://bjo9280.github.io/assets/images/2020-12-01/incrementalmean1.png)

 

# Incremental MC updates

![fig](https://bjo9280.github.io/assets/images/2020-12-01/incrementalmean2.png)

# Temporal-Difference Learning

* TD ë°©ë²•ë¡ ì€ ê²½í—˜ìœ¼ë¡œ ë¶€í„° ì§ì ‘ í•™ìŠµ
* Model Free ë°©ë²•ë¡  MDPì— ëŒ€í•œ ì •ë³´ë¥¼ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ”ë‹¤.
* ì—í”¼ì†Œë“œê°€ ëë‚˜ì§€ ì•Šì•„ë„ í•™ìŠµ ê°€ëŠ¥
* ì¶”ì¸¡ì„ ì¶”ì¸¡ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ë²•

# MC and TD

![fig](https://bjo9280.github.io/assets/images/2020-12-01/mcandtd.png)

# Example: Temporal-Difference

![fig](https://bjo9280.github.io/assets/images/2020-12-01/ex_td.png)

At k=1 (1,1) : 0.0 + 0.1(-0.04 + 0.9 (0.0) â€“ 0.0) = -0.004
At k=3 (1,2) : 0.0 + 0.1(-0.04 + 0.9 (-0.004) â€“ 0.0) = -0.00436
At k=4 (1,2) : -0.004 + 0.1 (-0.04 + 0.9 (-0.00436) â€“ (-0.004)) = -0.0079924

# ê° ë°©ë²•ë¡ ì˜ íŠ¹ì§•

* TDëŠ” ìµœì¢… ê²°ê³¼ë¥¼ ì•Œê¸° ì „ì— í•™ìŠµí•  ìˆ˜ ìˆë‹¤.
  * TDëŠ” ë§¤ ìŠ¤í…ë§ˆë‹¤ ì˜¨ë¼ì¸ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŒ.
  * ë°˜ë©´ MCëŠ” ì—í”¼ì†Œë“œê°€ ëë‚˜ì„œ ë¦¬í„´ì„ ì•Œê²Œ ë  ë•Œ ê¹Œì§€ ê¸°ë‹¤ë ¤ì•¼ í•¨
* Bias
  * ë¦¬í„´ G<sub>t</sub>ëŠ” ê°€ì¹˜ í•¨ìˆ˜ V<sub>Ï€</sub> (S<sub>t</sub>)ì˜ unbiased estimate
  * R<sub>t+1</sub>+Î³V<sub>Ï€</sub> (S<sub>(t+1)</sub>)ë„ unbiased
  * í•˜ì§€ë§Œ R<sub>t+1</sub>+Î³V(S<sub>(t+1)</sub>)ì€ biased
* Variance
  * TDíƒ€ê²Ÿì€ ë¦¬í„´ë³´ë‹¤ varianceê°€ í›¨ì”¬ ì‘ìŒ.
  * ë¦¬í„´ì€ ìˆ˜ë§ì€ ì•¡ì…˜, íŠ¸ëœì§€ì…˜, ë³´ìƒê³¼ ê´€ë ¨ì´ ë˜ì§€ë§Œ TD íƒ€ê²Ÿì€ í•œ ê°œì˜ ì•¡ì…˜, íŠ¸ëœì§€ì…˜, ë³´ìƒê³¼ ê´€ë ¨ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.

![fig](https://bjo9280.github.io/assets/images/2020-12-01/mctd.png)

# Dynamic Programming Backup

![fig](https://bjo9280.github.io/assets/images/2020-12-01/backup1.png)

# Monte-Carlo Backup

![fig](https://bjo9280.github.io/assets/images/2020-12-01/backup2.png)

# Temporal-Difference Backup

![fig](https://bjo9280.github.io/assets/images/2020-12-01/backup3.png)